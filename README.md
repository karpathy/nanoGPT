### Abstract

Multi Head Latent Attention Uses SVD for matrix compression. Our goal it to implement randomized-SVD instead of SVD, and make the process more efficient.

### Standard SVD Implementation

See https://github.com/cangokmen/CS599-Randomized-SVD/blob/master/SVD_README.md

### How to run?


### acknowledgements
MLA implementation is based on nanoGPT implementation of Karpathy.
https://github.com/karpathy/nanoGPT
