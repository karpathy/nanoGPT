# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

include "default_gpt2.conf"
config {

        out_dir = "out-shakespeare-char"
        eval_interval = 250
        log_interval = 10
        eval_iters = 200
        always_save_checkpoint = False

        wandb_project = "shakespeare-char"
        wandb_run_name = "nano-gpt"

        # data
        dataset = "shakespeare_char"
        gradient_accumulation_steps = 1
        batch_size = 64
        block_size = 256

        # baby GPT model :)
        n_layer = 6
        n_head = 6
        n_embd = 384
        dropout = 0.2

        # adamw optimizer
        learning_rate = 1e-3 # with baby networks can afford to go a bit higher
        max_iters = 5000 # total number of training iterations
        beta2 = 0.99

        # learning rate decay settings
        decay_lr = True # whether to decay the learning rate
        warmup_iters = 100 # not super necessary potentially
        lr_decay_iters = 5000 # make equal to max_iters usually
        min_lr = 1e-4 # learning_rate / 10 usually

        # on macbook also add
        # device = 'cpu'  # run on cpu only
        # compile = False # do not torch compile the model
}