# Minimal test configuration for bundestag_qwen15b_lora_mps to keep CLI tests fast.

[prepare]
dataset = "bundestag_qwen15b_lora_mps"
raw_dir = "./experiments/bundestag_qwen15b_lora_mps/raw/test_data"
dataset_dir = "./experiments/bundestag_qwen15b_lora_mps/datasets"
add_structure_tokens = false
doc_separator = "<DOC_SEP>"

[train.hf_model]
model_name = "Qwen/Qwen2.5-1.5B"
gradient_checkpointing = false
block_size = 32

[train.peft]
enabled = true
r = 8
lora_alpha = 16
lora_dropout = 0.05
bias = "none"
target_modules = ["q_proj","v_proj"]
extend_mlp_targets = false

[train.data]
dataset_dir = "./experiments/bundestag_qwen15b_lora_mps/datasets"
batch_size = 1
grad_accum_steps = 1
block_size = 32
sampler = "sequential"

[train.optim]
learning_rate = 0.001
weight_decay = 0.0
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

[train.schedule]
decay_lr = false
warmup_iters = 1
lr_decay_iters = 1
min_lr = 0.0001

[train.runtime]
out_dir = "./experiments/bundestag_qwen15b_lora_mps/out/test_run"
max_iters = 1
eval_interval = 1
eval_iters = 1
log_interval = 1
eval_only = false
always_save_checkpoint = false
seed = 42
device = "cpu"
dtype = "float32"
compile = false
ckpt_time_interval_minutes = 0
ckpt_atomic = true
best_smoothing_alpha = 0.0
early_stop_patience = 0
ema_decay = 0.0
save_merged_on_best = false
keep_last_n = 0

[sample.runtime]
out_dir = "./experiments/bundestag_qwen15b_lora_mps/out/test_run"
device = "cpu"
dtype = "float32"
compile = false
seed = 42

[sample.sample]
start = ""
num_samples = 1
max_new_tokens = 4
temperature = 0.8
top_k = 0
top_p = 0.9
