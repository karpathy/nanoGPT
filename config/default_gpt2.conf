#  configuration using pyhocon format.
#  it support comments, include, merge/inheritance, overwrite, variable substitution. dot notation etc.  )
#  https://github.com/chimpler/pyhocon
#
# The configuration is mostly the same as before except we add a root key ward: "config"
# this allows pyhocon to performs merge/inheritance function, this doesn't affect the normal usage
# as you will see.
# The usage will be
#   1) default configuration (default_gpt2.conf)
#    python train.py
#
#   2) use train_sharespeare_char.conf
#    python train.py -f config/train_sharespeare_char.conf
#
#   3) use train_sharespeare_char.conf but overwrite max_iters
#    python train.py -f config/train_sharespeare_char.conf -c max_iters = 500
#
# note configuration can't use dynamic content such as
#  run_name = "run" + str(time.time())
#  we have to put this in python code


# default config values designed to train a gpt2 (124M) on OpenWebText
config {
    out_dir = "out"
    eval_interval = 2000
    log_interval = 1
    eval_iters = 200

    eval_only = False # if True, script exits right after the first eval
    always_save_checkpoint = True # if True, always save a checkpoint after each eval
    init_from = "scratch" # "scratch" or "resume" or "gpt2*"

    wandb_project = "owt"
    wandb_run_name = "gpt2"
    wandb_log = False # disabled by default

    # data
    dataset = "openwebtext"
    gradient_accumulation_steps = 40  # 5 * 8 used to simulate larger batch sizes
    batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size
    block_size = 1024

    # model
    n_layer = 12
    n_head = 12
    n_embd = 768
    dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+
    bias = False # do we use bias inside LayerNorm and Linear layers?

    # defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency
    # this default value only applies if meta_vocab_size is not set.
    vocab_size = 50304 # default value
    vocab_size_reason = defaulting to vocab_size of GPT-2 to ${config.vocab_size} (50257 rounded up for efficiency)

    # adamw optimizer
    learning_rate = 6e-4 # max learning rate
    max_iters = 600000 # total number of training iterations     --> is this part of optimizer ?
    weight_decay = 1e-1
    beta1 = 0.9
    beta2 = 0.95
    grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0


    # learning rate decay settings
    decay_lr = True # whether to decay the learning rate
    warmup_iters = 2000 # how many steps to warm up for
    lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla
    min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

    # DDP settings
    backend  = "nccl" # "nccl", "gloo", etc.
    ddp_rank = -1 # default value
    ddp_rank = ${?RANK} # overwrite default value if RANK env variable is set

    seed_offset = 0 # default value
    seed_offset = ${?RANK} # each process gets a different seed in DDP

    ddp_world_size = 1 # default value
    ddp_world_size = ${?WORLD_SIZE} # overwrite with WORLD_SIZE env variable if env. variable is set

    ddp_local_rank = -1 # default value
    ddp_local_rank = ${?LOCAL_RANK} # overwrite with LOCAL_RANK env. variable if LOCAL_RANK is set


    # system
    # default to "cuda" or other you specified
    device = "cuda" # examples: "cpu", "cuda", "cuda:0", "cuda:1" etc., or try "mps" on macbooks
    device = ${?cuda:LOCAL_RANK} # if LOCAL_RANK is set, overwrite to new value

    dtype = "bfloat16" # "float32", "bfloat16", or "float16", the latter will auto implement a GradScaler
    compile = True # use PyTorch 2.0 to compile the model to be faster

}




