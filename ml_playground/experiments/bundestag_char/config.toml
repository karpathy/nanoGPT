# Training and sampling configuration for a char-level Transformer model.
# Notes on interplay:
# - Effective batch (in tokens) per optimizer update = batch_size * grad_accum_steps * block_size.
# - Memory roughly scales with n_layer * n_head and quadratically with block_size, and linearly with batch_size.
# - [train.data].block_size must be <= [train.model].block_size (model context capacity).
# - If [train.schedule].decay_lr = false, warmup/decay/min_lr are ignored.
# - Checkpoint selection uses ckpt_metric with optional smoothing and EMA if configured.

[prepare]
# Default locations used by preparers; resolved relative to this experiment dir
dataset_dir = "./datasets"
raw_dir = "./raw"
raw_text_path = "./raw/input.txt"
tokenizer_type = "char"

[train.model]
# mid-size model tuned for Apple Silicon (M2 Pro) with fp16 on MPS

# Number of Transformer layers (blocks); increases capacity and compute/memory cost.
n_layer = 4
# Number of attention heads per layer; must divide n_embd; more heads can help stability at added cost.
n_head = 8
# Embedding/hidden size; larger increases model capacity and compute.
n_embd = 384
# Maximum context length the model is built for (positional embeddings); must be >= [train.data].block_size.
# Increasing this raises memory/time roughly ~ O(block_size^2) per layer due to attention.
block_size = 256
# Dropout rate for regularization during training; set >0 to regularize, 0.0 for deterministic/fast runs.
dropout = 0.2
# Whether to include bias terms in linear/LayerNorm layers; false is common for GPT-style models.
bias = false
vocab_size = 256

[train.data]
# Folder containing the preprocessed dataset files referenced below.
# Tokenizer selection: "char" (default) or "word" for word-level vocabulary
tokenizer = "char"
# Character-level n-gram size used during dataset preparation (1 = single/chars, 3 = trigrams, etc.)
ngram_size = 1
# Micro-batch size per optimization step (per device); tuned for MPS memory
batch_size = 8
# Sequence length; ensure <= [train.model].block_size
block_size = 256
# Number of gradient accumulation steps to increase effective batch without extra memory
grad_accum_steps = 4
# Sampling policy: deterministic sequential coverage for char dataset
sampler = "random"

[train.optim]
# Optimizer hyperparameters (copied from defaults and tuned as needed)
learning_rate = 0.001
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

[train.schedule]
# Whether to apply a learning-rate schedule (typically warmup + decay).
decay_lr = true
# Number of warmup iterations from 0 -> learning_rate; ignored if decay_lr = false.
warmup_iters = 3_000
# Number of iterations over which to decay from learning_rate down toward min_lr; align with max_iters for cosine/poly schedules.
lr_decay_iters = 100_000
# Lower bound (floor) for the learning rate during decay; helps avoid stalling late in training.
min_lr = 0.00003

[train.runtime]
# Output directory for logs, checkpoints, and metadata for this training run.
out_dir = "./out"
# Total number of optimizer iterations to run (across all accumulation steps).
max_iters = 100_000
# How often (in iterations) to run evaluation/checkpoint logic.
eval_interval = 25
# Number of evaluation batches to average per evaluation; higher reduces noise but takes longer.
eval_iters = 10
# How often (in iterations) to log training metrics.
log_interval = 25
# If true, only runs evaluation and exits (no training updates); useful for validating or benchmarking.
eval_only = false
# If true, save a checkpoint on every eval (last and/or best/top-k as configured); increases disk usage.
always_save_checkpoint = false
# Random seed for reproducibility (data shuffling, init, sampling).
seed = 1337
# Compute device backend: "cuda" (NVIDIA), "mps" (Apple Silicon), or "cpu".
device = "mps"
# Numeric precision for tensors and parameters; affects speed/memory vs. stability/accuracy.
dtype = "bfloat16"
# If true, compile/optimize the model/graph (when supported); speeds up but increases startup time.
compile = false

# Prefer reading the best checkpoint during sampling by default
[sample.runtime.checkpointing]
read_policy = "best"

[sample.runtime]
# Output directory to read checkpoints from (and optionally write samples/logs to).
# Typically should match the training run's out_dir you want to sample from.
out_dir = "./out"
# Iteration controls are usually inert for pure sampling, but kept for API symmetry.
max_iters = 0
# Evaluation cadence placeholders (often unused in pure sampling flows).
eval_interval = 1
# Number of evaluation batches during sampling flows (often unused).
eval_iters = 1
# Logging cadence during sampling (if the driver logs progress).
log_interval = 1
# If true, run evaluation-only behavior in sampling loop implementations that support it.
eval_only = false
# Whether to save artifacts during sampling (e.g., generated text dumps); false to avoid clutter.
always_save_checkpoint = false
# Seed to make generation reproducible for a fixed prompt and settings.
seed = 1337
# Device to run generation on; "cpu" is fine for small char-level models, use "cuda"/"mps" if available.
device = "mps"
# Numeric precision during sampling; can be lower precision if supported to speed up generation.
dtype = "bfloat16"
# Whether to compile/optimize the model for inference (if supported); usually false for quick runs.
compile = false

[sample.sample]
# Prompt to prime the model; generation continues after this text.
start = "\n(Beifall bei der FDP): \n"
# Number of independent samples to generate for the same prompt/settings.
num_samples = 2
# Maximum number of new tokens to generate after the prompt.
max_new_tokens = 1024
# Softens or sharpens the probability distribution before sampling; higher = more random, 0 = greedy/argmax.
temperature = 0.8
# Sample only from the top-K most likely tokens at each step; lower K increases coherence, higher K increases diversity.
# Interplay: lower temperature + small top_k -> focused, deterministic-like; higher temperature + large top_k -> diverse/creative.
top_k = 100


# Export config is not supported by the strict schema at the top level; removed for compatibility
