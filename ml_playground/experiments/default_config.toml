# Shared CPU-friendly defaults for all experiments.
#
# Use these as a minimal, safe baseline; experiments may override any subset of
# settings they need. Paths (dataset_dir, out_dir) are resolved
# relative to the current experiment.
#
# Common guidance and notes:
# - Many knobs are inert at their defaults (e.g., ema_decay=0.0, best_smoothing_alpha=0.0,
#   ckpt_top_k=0, early_stop_patience=0). They exist for completeness but do nothing until set.
# - Effective batch per optimizer update is batch_size * grad_accum_steps * block_size tokens.
# - Memory scales roughly with n_layer and block_size^2 per layer; choose conservatively on CPU.
# - If [train.schedule].decay_lr = false, warmup/decay/min_lr are ignored.
# - Key constraint: [train.data].block_size must be <= [train.model].block_size.
#
# Experiments can override any subset of keys in their config TOML; at startup the
# loader logs where each effective value came from (default vs experiment override vs env override).

[train.model]
# Small GPT-like model suitable for CPU experiments (reduce memory/compute).
# Memory scales with block_size^2 per layer. Ensure [train.data].block_size <= [train.model].block_size.
n_layer = 6
n_head = 6
n_embd = 384
block_size = 256
# Dropout is disabled for determinism and simplicity on CPU.
dropout = 0.0
bias = false

[train.data]
# Paths are used as-is (absolute or relative), no rewriting.
# The default directory can be assumed to exist/be created by prepare steps.
dataset_dir = "./datasets"
train_bin = "train.bin"
val_bin = "val.bin"
meta_pkl = "meta.pkl"
# Character-level n-gram size for datasets that use char/ngram tokenization (1 keeps legacy behavior)
ngram_size = 1
# Effective batch tokens per optimizer step: batch_size * grad_accum_steps * block_size
batch_size = 8
block_size = 256
grad_accum_steps = 1

[train.optim]
# Base learning rate used by the optimizer; interacts with schedule below if decay_lr = true.
learning_rate = 0.001
# L2-style weight decay for regularization; often excluded for LayerNorm/bias in AdamW.
weight_decay = 0.1
# Adam/AdamW beta1 (momentum) parameter; higher retains more past gradients.
beta1 = 0.9
# Adam/AdamW beta2 (variance) parameter; higher smooths more but reacts slower to changes.
beta2 = 0.95
# Global gradient norm clipping threshold to prevent exploding gradients (0 disables if supported).
grad_clip = 1.0

[train.schedule]
# Enable learning-rate schedule (warmup + decay). If false, other fields are ignored.
decay_lr = true
# Number of warmup iterations from 0 -> learning_rate.
warmup_iters = 200
# Iterations over which LR decays from learning_rate down toward min_lr.
# Consider interplay with [train.runtime].max_iters; if much smaller, LR stays at min for long.
lr_decay_iters = 20000
# Lower bound (floor) for the learning rate during decay.
min_lr = 0.0001

[train.runtime]
# Paths are used as-is (absolute or relative), no rewriting.
out_dir = "./out/default_run"
max_iters = 100000
# Cadence tradeoffs: lower eval_interval/eval_iters increases feedback but adds overhead.
# always_save_checkpoint ensures the last/best are persisted; ckpt_top_k=0 disables top-k pruning.
eval_interval = 1000
eval_iters = 100
log_interval = 10
eval_only = false
always_save_checkpoint = true
seed = 1337
# CPU-first defaults
device = "cpu"
dtype = "float32"
compile = false
# TensorBoard logging (framework-level); set false to disable
# Default is true to keep logging enabled unless explicitly disabled.
tensorboard_enabled = true
# Checkpoint policy (several knobs are inert at defaults)
ckpt_last_filename = "ckpt_last.pt"
ckpt_best_filename = "ckpt_best.pt"
ckpt_top_k = 0
ckpt_metric = "val_loss"
ckpt_greater_is_better = false
ckpt_atomic = true
ckpt_write_metadata = true
ckpt_time_interval_minutes = 0
# Smoothing/early stop and EMA (inert at 0.0 / 0)
best_smoothing_alpha = 0.0
early_stop_patience = 0
ema_decay = 0.0

[sample.runtime]
# Use the same default out_dir to find checkpoints by default (fully overridable).
out_dir = "./out/default_run"
# CPU-first sampling defaults
device = "cpu"
dtype = "float32"
compile = false
seed = 1337

[sample.sample]
# Minimal, conservative sampling defaults. Either omit top_k or set a neutral default.
start = ""
num_samples = 1
max_new_tokens = 128
temperature = 0.9
# Neutral top_k (0 disables top-k filtering)
top_k = 0
